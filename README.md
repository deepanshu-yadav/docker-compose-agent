

# Pre Requisites
## Installation of docker 

Install docker from [here](https://docs.docker.com/engine/install/)

## Installation of docker-compose

Install docker compose from [here](https://docs.docker.com/compose/install/)

## Installation of ollama 

install ollma from [here](https://ollama.com/download)

## Install offline models of deep seek
In one terminal keep running the following
```
ollama serve 
```

Also download these models
```
ollama pull deepseek-r1:1.5b 
ollama pull deepseek-r1:7b
ollama pull deepseek-r1:32b
```

After downloading test the models using
by first downloading curl and then use

```
curl http://localhost:11434/api/generate -d '{
  "model": "deepseek-r1:1.5b",
  "prompt": "one word about me just one word",
  "stream": false
}'
```

Make sure it works and it keeps on running at `11434`

## Now install requirements.txt 

Which ever python  installation you are running be it virtual environment or anaconda 
install `pip install -r requirements.txt`

## Finally 

```
streamlit run app.py
```

It will redirect you to `http://localhost:8501`

# Features
There are various features currently in this agent.
## Execution Enviornment
We can run the generated files in the chat interface itself.
![Env](./images/execution_env.png)

## Model selection
We can select different models. 
![Model](./images/model_selection.png)

## Chat interface
We can interact with AI to deploy our solutions in the chat interface itself.
![Chat](./images/chat_interface.png)

## Deployment Environment creation
As soon as the response is created all the genrated files are saved in file system in the provided 
directory.
![creation](./images/execution_env.png)

## Feedback
If a deployment is unsuccessful we can get feeback from the process to forward to the AI to look 
at it.
![feed](./images/feedback.png)

## Monitoring
If a service is running we can continously monitor it and inform the AI about any failure. 
![monitoring](./images/monitoring.png)

# Future plans

## Offline vs Online

This is working only for offline deepseek model. Online inference with deepseek are much
better and faster. So support for chatting with Deepseek API.
Offline speed is very slow on my mac for 32b model.

## Another AI agent for manipulating this services.
1. Generating curl commands for interacting with various services.
2. Getting metrics, logs from the services, resource usage and making a sense of the data fetched and also closely monitoring 
these services. 
3. Devising strategies for recovery in case of failure. 
4. Increase load on services for stress testing. 
5. Testing of backup restore of databses if databases are involved.
6. We are parsing the files generated by the model and saving them. The model could be made to generate the code for
environement creation as well.

## Saving working configuration in database.
Saving the configurations generated in database ( implementing RAG). 

## Moving to kubernetes after this. 
Repeating all of this kubenetes. 

## Reinforcement learning
The feeback information can be used to update the model parameters. 
But more realistc one would be first storing the monitoring information to RAG 
and then thinking of reinforcement learning. 


# Contributions

Contributions are welcome. You can reach out to me on mail (awsrohanyadav@gmail.com) or my [linkedin](https://www.linkedin.com/in/deepanshu-yadav-8b324092/) or open an issue or pull request. 

# Credits

A lot of chat interface code has been borrowed from [here](https://github.com/krishnaik06/Gen-AI-With-Deep-Seek-R1)

Thanks [@krishnaik Buddy.](https://github.com/krishnaik06)

And [@dcrey7](https://github.com/dcrey7)

Thank you buddies.

